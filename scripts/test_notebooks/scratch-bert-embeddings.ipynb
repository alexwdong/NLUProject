{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open(\"../../header.py\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import BertTokenizer,BertModel\n",
    "from datasets import load_from_disk,load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segments_list(cutoff_indices, sentence_list,tokenizer):\n",
    "    '''\n",
    "    Input:\n",
    "        cutoff_indices: a list of cutoff indices. each index should be in the range of 0 to n-1, where n=len(sentence_list)\n",
    "        sentence_list: a list of sentences from sent_tokenize\n",
    "        tokenizer: the tokenizer for the model.\n",
    "    Returns:\n",
    "        segments_list: a list of 3-tuples of type BatchEncoding. This 3-tuple is the output of encode_plus\n",
    "    '''\n",
    "    segments_list = []\n",
    "    #If cutoff indices is an empty list, means we don't split at all. then all the sentences get joined into one segment\n",
    "    if len(cutoff_indices) == 0: \n",
    "        segment = \"\".join(sentence_list).lower()\n",
    "        encoded_segment = tokenizer.encode_plus(segment,add_special_tokens=True,padding='max_length',max_length=512,truncation=True,return_tensors='pt')\n",
    "        segments_list.append(encoded_segment)\n",
    "        return segments_list\n",
    "    #Make first n-1 splits\n",
    "    start_idx = 0\n",
    "    segments_list = []\n",
    "    for split_idx in cutoff_indices: \n",
    "        grouped_sentences_list = sentence_list[start_idx:split_idx+1] \n",
    "        segment = \"\".join(grouped_sentences_list).lower()\n",
    "        encoded_segment = tokenizer.encode_plus(segment,add_special_tokens=True,padding='max_length',max_length=512,truncation=True,return_tensors='pt')\n",
    "        segments_list.append(encoded_segment)\n",
    "        start_idx = split_idx+1\n",
    "    # make last split\n",
    "    grouped_sentences_list = sentence_list[start_idx:] \n",
    "    segment = \"\".join(grouped_sentences_list).lower()\n",
    "    encoded_segment = tokenizer.encode_plus(segment,add_special_tokens=True,padding='max_length',max_length=512,truncation=True, return_tensors='pt')\n",
    "    segments_list.append(encoded_segment)\n",
    "    #Return \n",
    "    return segments_list\n",
    "\n",
    "class SegmentDataset(Dataset):\n",
    "    def __init__(self, dataset_list,configs, label_to_cutoff_indices_dict,tokenizer):\n",
    "        self.label_to_label_idx_dict = {}\n",
    "        for ii,label in enumerate(configs):\n",
    "            self.label_to_label_idx_dict[label]=ii\n",
    "        \n",
    "        self.data = []\n",
    "        for label, sub_dataset in dataset_list:\n",
    "            print('applying splits for label: ',label)\n",
    "            cutoff_indices_dict = label_to_cutoff_indices_dict[label]\n",
    "            for ii, entry in enumerate(sub_dataset):\n",
    "                context = entry['text']\n",
    "                sentence_list = sent_tokenize(context)\n",
    "                cutoff_indices = cutoff_indices_dict[ii]\n",
    "                segments_list = create_segments_list(cutoff_indices,sentence_list,tokenizer)\n",
    "                data_entry = (self.label_to_label_idx_dict[label],segments_list)\n",
    "                self.data.append(data_entry)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    " \n",
    "    def __getitem__(self,idx):\n",
    "        return(self.data[idx])\n",
    "    \n",
    "class OnTheFlyDataset(Dataset):\n",
    "    def __init__(self, encode_plus_out_list):\n",
    "        self.encode_plus_out_list = encode_plus_out_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.encode_plus_out_list)\n",
    " \n",
    "    def __getitem__(self,idx):\n",
    "        return(self.encode_plus_out_list[idx])\n",
    "def squeeze_tensors(batch):\n",
    "    '''\n",
    "    batch has four dimensions (b_size,useless,useless, 512 (representing padded tokens))\n",
    "    We want to squeeze the second and third dimensions\n",
    "    '''\n",
    "    batch['input_ids'] = batch['input_ids'].squeeze(axis=1).squeeze(axis=1)\n",
    "    batch['token_type_ids'] = batch['token_type_ids'].squeeze(axis=1).squeeze(axis=1)\n",
    "    batch['attention_mask'] = batch['attention_mask'].squeeze(axis=1).squeeze(axis=1)\n",
    "    return batch\n",
    "\n",
    "\n",
    "# ArgParse\n",
    "parser = argparse.ArgumentParser(description='Takes \"label_to_cutoff_indices\" pickle file, and creates BERT encoded segments')\n",
    "\n",
    "parser.add_argument('-t','--threshold',help='threshold. This isnt technically required, because the threshold is already used in the previous script (make_cutoff_indices), but this helps for loading the correct file.', required=True)\n",
    "parser.add_argument('-m', '--mode', help='what dataset are we using (currently only newsgroup is accepted)', default='newsgroup')\n",
    "parser.add_argument('-d', '--data_dir', help='path_to_data_dir', required=True)\n",
    "parser.add_argument('-p', '--processed_dir', help = 'path to processed_dir, which contains the label_to_cutoff_indices pickle file and also where the output of this script will be stored', required=True)\n",
    "args = vars(parser.parse_args())\n",
    "\n",
    "threshold = float(args['threshold'])\n",
    "mode = args['mode']\n",
    "data_dir = args['data_dir']\n",
    "processed_dir = args['processed_dir']\n",
    "\n",
    "if mode == 'newsgroup':\n",
    "    newsgroup_configs = ['bydate_alt.atheism',\n",
    "                         'bydate_comp.graphics',\n",
    "                         'bydate_comp.os.ms-windows.misc',\n",
    "                         'bydate_comp.sys.ibm.pc.hardware',\n",
    "                         'bydate_comp.sys.mac.hardware',\n",
    "                         'bydate_comp.windows.x',\n",
    "                         'bydate_misc.forsale',\n",
    "                         'bydate_rec.autos',\n",
    "                         'bydate_rec.motorcycles',\n",
    "                         'bydate_rec.sport.baseball',\n",
    "                         'bydate_rec.sport.hockey',\n",
    "                         'bydate_sci.crypt',\n",
    "                         'bydate_sci.electronics',\n",
    "                         'bydate_sci.med',\n",
    "                         'bydate_sci.space',\n",
    "                         'bydate_soc.religion.christian',\n",
    "                         'bydate_talk.politics.guns',\n",
    "                         'bydate_talk.politics.mideast',\n",
    "                         'bydate_talk.politics.misc',\n",
    "                         'bydate_talk.religion.misc']\n",
    "    splits = ['train','test']\n",
    "\n",
    "# Start Script\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Start Script\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using device:', device)\n",
    "\n",
    "    #Additional Info when using cuda\n",
    "    if device.type == 'cuda':\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    bert_model= BertModel.from_pretrained('bert-base-uncased')\n",
    "    bert_model.eval()\n",
    "    bert_model.to(device)\n",
    "    for split in splits:\n",
    "        dataset_list = []\n",
    "        #Create (train, val or test) Dataset list \n",
    "        for config in newsgroup_configs:\n",
    "            subset_path = data_dir + split + '/'+ config\n",
    "            dataset_list.append((config,load_from_disk(subset_path)))\n",
    "        \n",
    "        # Load the label_to_cutoff_indices pkl file, which contains the sentence splits for each long document.\n",
    "        label_to_cutoff_indices_file = \\\n",
    "            processed_dir + \\\n",
    "            split + '/label_to_cutoff_indices_' + str(threshold) + '.pkl'\n",
    "        with open(label_to_cutoff_indices_file, 'rb') as handle:\n",
    "            label_to_cutoff_indices_dict = pickle.load(handle)\n",
    "\n",
    "\n",
    "        #Create a Segment Dataset which contains tuples of (label - int, list of segments - list of 3-tuple which is output from tokenizer.encode_plus))\n",
    "        split_set = SegmentDataset(dataset_list,newsgroup_configs,label_to_cutoff_indices_dict,tokenizer)\n",
    "        split_loader = DataLoader(split_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "        \n",
    "        #Initialize bert_encoded_segments_list, this will contain the output that we want to dump\n",
    "        bert_encoded_segments_list = []\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(split_loader):\n",
    "                label =  batch[0]\n",
    "                encoded_segments = batch[1]\n",
    "                onthefly_dataset = OnTheFlyDataset(encoded_segments)\n",
    "                onthefly_loader = DataLoader(onthefly_dataset, batch_size=4, shuffle=False, pin_memory=True)\n",
    "                batch_encoded_seg_list = []\n",
    "                for ii, small_batch in enumerate(onthefly_loader):\n",
    "                    small_batch = squeeze_tensors(small_batch)\n",
    "                    batch_input_ids = small_batch['input_ids'].to(device)\n",
    "                    batch_token_type_ids = small_batch['token_type_ids'].to(device)\n",
    "                    batch_attention_mask = small_batch['attention_mask'].to(device)\n",
    "                    out = bert_model(batch_input_ids, batch_token_type_ids, batch_attention_mask)\n",
    "                    # out['last_hidden_state'] is bsize x seq_len x embedding_size. We want to take only the embedding\n",
    "                    # which corresponds to the CLS token.\n",
    "                    sub_bert_encoded_segments = out['last_hidden_state'][:,0,:] #take only the first\n",
    "                    batch_encoded_seg_list.append(sub_bert_encoded_segments)\n",
    "                bert_encoded_segments = torch.cat(batch_encoded_seg_list)\n",
    "                bert_encoded_segments_list.append((label,bert_encoded_segments.cpu()))\n",
    "        file_name = 'bert_encoded_segments_list_'\n",
    "        with open(processed_dir+ split+'/' + file_name + str(threshold) +'.pkl', 'wb') as handle:\n",
    "            pickle.dump(bert_encoded_segments_list, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
