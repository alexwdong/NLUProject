{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open(\"../../header.py\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_from_disk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import BertTokenizer,BertForNextSentencePrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextDataset(Dataset):\n",
    "    def __init__(self, sentence_pair_list):\n",
    "        self.sentence_pair_list = sentence_pair_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence_pair_list)\n",
    " \n",
    "    def __getitem__(self,idx):\n",
    "        return(self.sentence_pair_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probabilities_on_text_w_NSP(nsp_model, text, tokenizer, device):\n",
    "    '''\n",
    "    Returns a sequence of probabilities which represent confidence that the next sentence is part of the same segment\n",
    "    \n",
    "    If text has n sentences, then prob_seq has n-1 probabilities. (If text has 1 sentence, prob_seq is [], the empty list.)\n",
    "    The ii index of prob seq represents the NSP confidence of the ii and ii+1 sentences in text.\n",
    "    Probabilities closer to 1 indicate confidence, Probabilities closer to 0 indicate no confidence.\n",
    "    \n",
    "    '''\n",
    "    sentence_list = sent_tokenize(text)\n",
    "    over_length_indices = []\n",
    "    sentence_pair_list=[]\n",
    "    indices_to_be_processed=[]\n",
    "    #Create Sentence pair list\n",
    "    if len(sentence_list)==1:\n",
    "        return [],sentence_list # Return empty list for probs\n",
    "    \n",
    "    for ii in range(0,len(sentence_list)-1):\n",
    "        sentence_1 = sentence_list[ii]\n",
    "        sentence_2 = sentence_list[ii+1]\n",
    "\n",
    "        #Encode temporarily, just to count\n",
    "        encoded = tokenizer.encode_plus(sentence_1, text_pair=sentence_2, return_tensors='pt')\n",
    "        if encoded['input_ids'].shape[1] > 512: # If two sentences are too long, just split them\n",
    "            over_length_indices.append(ii)\n",
    "        else:# add to list to be processed\n",
    "            indices_to_be_processed.append(ii)\n",
    "            sentence_pair_list.append([sentence_1,sentence_2])\n",
    "    # Now, begin calculating probabilities\n",
    "    with torch.no_grad():\n",
    "        # Load into a dataset and dataloader. We do this for speed\n",
    "        context_dataset = ContextDataset(sentence_pair_list)\n",
    "        context_loader = DataLoader(context_dataset, batch_size=128, shuffle=False, pin_memory=True)\n",
    "        probs_list = [] #Will be list of tensors\n",
    "        for batch_idx,batch in enumerate(context_loader):\n",
    "            if len(batch)==0:\n",
    "                continue\n",
    "            # I swear to god this is a legitimate pytorch bug, but we need to reorganize the batch from the dataloader. Whatever\n",
    "            batch_fixed = [(s1,s2) for s1,s2 in zip(batch[0], batch[1])] \n",
    "            # Batch encode\n",
    "            sentence_pairs = tokenizer.batch_encode_plus(batch_fixed, return_tensors='pt',padding=True)\n",
    "            # Run through the model\n",
    "            sentence_pairs.to(device)\n",
    "            logits = nsp_model(**sentence_pairs)[0]\n",
    "            # Get Probability of next sentence.\n",
    "            probs = softmax(logits, dim=1)\n",
    "            probs = probs[:,0]\n",
    "            # Add to list of tensors\n",
    "            probs_list.append(probs.cpu())\n",
    "            \n",
    "        #Cat the list of tensors to get a bsize x sequence_length tensors\n",
    "        if len(probs_list)  == 0:\n",
    "            all_probs = []\n",
    "        else:\n",
    "            all_probs = list(torch.cat(probs_list))\n",
    "    # Now, we need to sort the probabilities. Some of probabilities are coming from over_length_indices, some of them are coming from indices_to_be_processed\n",
    "    # We'll zip, then sort, then take the sorted probs.\n",
    "    indices = over_length_indices + indices_to_be_processed\n",
    "    one_probs = [1]*len(over_length_indices)\n",
    "    probs = one_probs + all_probs\n",
    "    probs = [x for _, x in sorted(zip(indices, probs))]\n",
    "    # Return probabilities, and also return sentence list for use later as well\n",
    "    return probs, sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Quadro RTX 8000\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# ArgParse\n",
    "splits = ['train','test']\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print_cuda_info(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "nsp_model = BertForNextSentencePrediction.from_pretrained('prajjwal1/bert-small')\n",
    "nsp_model.eval()\n",
    "nsp_model.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"train\"\n",
    "\n",
    "dataset_list = []\n",
    "\n",
    "for config in newsgroup_configs: #loop over labels\n",
    "        subset_path = RAW_DIR(f'20news/{split}/{config}')\n",
    "        dataset_list.append((config, load_from_disk(subset_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bydate_alt.atheism',\n",
       " Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 480\n",
       " }))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, sub_dataset in dataset_list: #Loop over labels\n",
    "        qid_struct = {}\n",
    "        for ii, entry in enumerate(sub_dataset):# Loop over data entries with the same label\n",
    "            context = entry['text']\n",
    "            prob_seq , _ = get_probabilities_on_text_w_NSP(nsp_model, context, tokenizer, device)\n",
    "            qid_struct[ii] = prob_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_struct[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
