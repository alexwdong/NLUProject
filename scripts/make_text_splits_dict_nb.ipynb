{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import json\n",
    "import torch\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from torch.nn.functional import softmax\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer,BertForNextSentencePrediction\n",
    "import pickle\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextDataset(Dataset):\n",
    "    def __init__(self, sentence_pair_list):\n",
    "        self.sentence_pair_list = sentence_pair_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence_pair_list)\n",
    " \n",
    "    def __getitem__(self,idx):\n",
    "        return(self.sentence_pair_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probabilities_on_text_w_NSP(nsp_model, text, tokenizer, device):\n",
    "    '''\n",
    "    Returns a sequence of probabilities which represent confidence that the next sentence is part of the same segment\n",
    "    \n",
    "    If text has n sentences, then prob_seq has n-1 probabilities. (If text has 1 sentence, prob_seq is [], the empty list.)\n",
    "    The ii index of prob seq represents the NSP confidence of the ii and ii+1 sentences in text.\n",
    "    Probabilities closer to 1 indicate confidence, Probabilities closer to 0 indicate no confidence.\n",
    "    \n",
    "    '''\n",
    "    sentence_list = sent_tokenize(text)\n",
    "    over_length_indices = []\n",
    "    sentence_pair_list=[]\n",
    "    indices_to_be_processed=[]\n",
    "    #Create Sentence pair list\n",
    "    if len(sentence_list)==1:\n",
    "        return [],sentence_list # Return empty list for probs\n",
    "    \n",
    "    for ii in range(0,len(sentence_list)-1):\n",
    "        sentence_1 = sentence_list[ii]\n",
    "        sentence_2 = sentence_list[ii+1]\n",
    "\n",
    "        #Encode temporarily, just to count\n",
    "        encoded = tokenizer.encode_plus(sentence_1, text_pair=sentence_2, return_tensors='pt')\n",
    "        if encoded['input_ids'].shape[1] > 512: # If two sentences are too long, just split them\n",
    "            over_length_indices.append(ii)\n",
    "        else:# add to list to be processed\n",
    "            indices_to_be_processed.append(ii)\n",
    "            sentence_pair_list.append([sentence_1,sentence_2])\n",
    "    # Now, begin calculating probabilities\n",
    "    with torch.no_grad():\n",
    "        # Load into a dataset and dataloader. We do this for speed\n",
    "        context_dataset = ContextDataset(sentence_pair_list)\n",
    "        context_loader = DataLoader(context_dataset, batch_size=128, shuffle=False, pin_memory=True)\n",
    "        probs_list = [] #Will be list of tensors\n",
    "        for batch_idx,batch in enumerate(context_loader):\n",
    "            if len(batch)==0:\n",
    "                continue\n",
    "            # I swear to god this is a legitimate pytorch bug, but we need to reorganize the batch from the dataloader. Whatever\n",
    "            batch_fixed = [(s1,s2) for s1,s2 in zip(batch[0], batch[1])] \n",
    "            # Batch encode\n",
    "            sentence_pairs = tokenizer.batch_encode_plus(batch_fixed, return_tensors='pt',padding=True)\n",
    "            # Run through the model\n",
    "            sentence_pairs.to(device)\n",
    "            logits = nsp_model(**sentence_pairs)[0]\n",
    "            # Get Probability of next sentence.\n",
    "            probs = softmax(logits, dim=1)\n",
    "            probs = probs[:,0]\n",
    "            # Add to list of tensors\n",
    "            probs_list.append(probs.cpu())\n",
    "            \n",
    "        #Cat the list of tensors to get a bsize x sequence_length tensors\n",
    "        if len(probs_list)  == 0:\n",
    "            all_probs = []\n",
    "        else:\n",
    "            all_probs = list(torch.cat(probs_list))\n",
    "    # Now, we need to sort the probabilities. Some of probabilities are coming from over_length_indices, some of them are coming from indices_to_be_processed\n",
    "    # We'll zip, then sort, then take the sorted probs.\n",
    "    indices = over_length_indices + indices_to_be_processed\n",
    "    one_probs = [1]*len(over_length_indices)\n",
    "    probs = one_probs + all_probs\n",
    "    probs = [x for _, x in sorted(zip(indices, probs))]\n",
    "    # Return probabilities, and also return sentence list for use later as well\n",
    "    return probs, sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Quadro RTX 8000\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "mode = '20news'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03089c5b5c1d4d2db83227f3d988ee50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=286.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82fe3f3b6f444808aaa0e4421f19562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=116270890.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf76f1a1dc7437fbc58749149bd9b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nsp_model = BertForNextSentencePrediction.from_pretrained('prajjwal1/bert-small')\n",
    "nsp_model.eval()\n",
    "nsp_model.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'wikihop':\n",
    "     #dataset = load_from_disk('/home/adong/School/NLUProject/data/trivia_qa_rc_tiny')\n",
    "    #dataset = load_from_disk(r'\\\\wsl$\\Ubuntu-20.04\\home\\jolteon\\NLUProject\\data\\trivia_qa_rc_tiny')\n",
    "    dataset = load_from_disk('/scratch/awd275/NLU_data/trivia_qa_rc/')\n",
    "    qid_struct = {}\n",
    "    for key in dataset.keys():\n",
    "        sub_dataset = dataset[key]\n",
    "        qid_struct = {}\n",
    "        for ii, entry  in enumerate(sub_dataset):\n",
    "            #if ii ==5:\n",
    "            #    break\n",
    "            print('started: ',str(ii))\n",
    "\n",
    "            if len(entry['entity_pages']['wiki_context'])==0:\n",
    "                wiki_context_probs = None\n",
    "            else:\n",
    "                wiki_context_probs = []\n",
    "                for context in entry['entity_pages']['wiki_context']:\n",
    "                    prob_seq , _ = get_probabilities_on_text_w_NSP(nsp_model, context, tokenizer, device)\n",
    "                    wiki_context_probs.append(prob_seq)\n",
    "\n",
    "            if len(entry['search_results']['search_context']) == 0:\n",
    "                 search_context_probs = None\n",
    "            else:\n",
    "                search_context_probs = []\n",
    "                for context in entry['search_results']['search_context']:\n",
    "\n",
    "                    prob_seq , _ = get_probabilities_on_text_w_NSP(nsp_model, context, tokenizer, device)\n",
    "                    search_context_probs.append(prob_seq)\n",
    "\n",
    "            qid_struct[entry['question_id']] = (wiki_context_probs,search_context_probs)\n",
    "        file_name = key + '_qid_struct.pkl'\n",
    "\n",
    "        with open(\"/scratch/awd275/NLU_data/\" + file_name, 'wb') as handle:\n",
    "            pickle.dump(qid_struct, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "elif mode == '20news':\n",
    "    newsgroup_configs = ['bydate_alt.atheism',\n",
    "                     'bydate_comp.graphics',\n",
    "                     'bydate_comp.os.ms-windows.misc',\n",
    "                     'bydate_comp.sys.ibm.pc.hardware',\n",
    "                     'bydate_comp.sys.mac.hardware',\n",
    "                     'bydate_comp.windows.x',\n",
    "                     'bydate_misc.forsale',\n",
    "                     'bydate_rec.autos',\n",
    "                     'bydate_rec.motorcycles',\n",
    "                     'bydate_rec.sport.baseball',\n",
    "                     'bydate_rec.sport.hockey',\n",
    "                     'bydate_sci.crypt',\n",
    "                     'bydate_sci.electronics',\n",
    "                     'bydate_sci.med',\n",
    "                     'bydate_sci.space',\n",
    "                     'bydate_soc.religion.christian',\n",
    "                     'bydate_talk.politics.guns',\n",
    "                     'bydate_talk.politics.mideast',\n",
    "                     'bydate_talk.politics.misc',\n",
    "                     'bydate_talk.religion.misc']\n",
    "    splits = ['train','test']\n",
    "    for split in splits: # Loop over train test\n",
    "        dataset_list = []\n",
    "        for config in newsgroup_configs: #loop over labels\n",
    "            subset_path = r'\\\\wsl$\\Ubuntu-20.04\\home\\jolteon\\NLUProject\\data\\20news\\\\'+ split+ '\\\\'+ config\n",
    "            dataset_list.append((config,load_from_disk(subset_path)))\n",
    "\n",
    "        for label, sub_dataset in dataset_list: #Loop over labels\n",
    "            qid_struct = {}\n",
    "            for ii, entry in enumerate(sub_dataset):# Loop over data entries with the same label\n",
    "                context = entry['text']\n",
    "                prob_seq , _ = get_probabilities_on_text_w_NSP(nsp_model, context, tokenizer, device)\n",
    "                qid_struct[ii] = prob_seq\n",
    "            file_name = label + '_qid_struct.pkl'\n",
    "            with open(r\"\\\\wsl$\\Ubuntu-20.04\\home\\jolteon\\NLUProject\\data\\20news\\processed\\\\\" + split + '\\\\' +file_name, 'wb') as handle:\n",
    "                pickle.dump(qid_struct, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
