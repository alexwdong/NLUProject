{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45db3571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import json\n",
    "import torch\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from torch.nn.functional import softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff820fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,BertForNextSentencePrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e837bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probabilities_on_text_w_NSP(nsp_model, text, tokenizer):\n",
    "    '''\n",
    "    Returns a sequence of probabilities which represent confidence that the next sentence is part of the same segment\n",
    "    \n",
    "    If text has n sentences, then prob_seq has n-1 probabilities. \n",
    "    The ii index of prob seq represents the NSP confidence of the ii and ii+1 sentences in text.\n",
    "    Probabilities closer to 1 indicate confidence, Probabilities closer to 0 indicate no confidence.\n",
    "     \n",
    "    '''\n",
    "    #Create sentence list\n",
    "    sentence_list = sent_tokenize(text)\n",
    "    prob_seq = []\n",
    "    #Iterate over all sequential pairs\n",
    "    for ii in range(0,len(sentence_list)-1):\n",
    "        sentence_1 = sentence_list[ii]\n",
    "        sentence_2 = sentence_list[ii+1]\n",
    "        \n",
    "        #Encode\n",
    "        encoded = tokenizer.encode_plus(sentence_1, text_pair=sentence_2, return_tensors='pt')\n",
    "        \n",
    "        #print(encoded['input_ids'].shape[1])\n",
    "        if encoded['input_ids'].shape[1] > 512: # If two sentences are too long, just split them\n",
    "            prob_seq.append(0)\n",
    "        else:\n",
    "            #Not too long, pass through the model and get a probability\n",
    "            with torch.no_grad():\n",
    "                logits = nsp_model(**encoded)[0]\n",
    "\n",
    "            probs = softmax(logits, dim=1)\n",
    "            prob_seq.append(probs[0][0])\n",
    "    #End for loop\n",
    "    return prob_seq,sentence_list\n",
    "\n",
    "def get_tokens_per_sentence_list(tokenizer,sentence_list):\n",
    "    tokens_per_sentence_list = [len(tokenizer.encode(sentence)) for sentence in sentence_list]\n",
    "    return tokens_per_sentence_list\n",
    "\n",
    "def apply_threshold(prob_seq,tokens_per_sentence_list,threshold):\n",
    "    # Initialize\n",
    "    cutoff_indices = []\n",
    "    running_length = tokens_per_sentence_list[0]\n",
    "    # \n",
    "    for ii,prob in enumerate(prob_seq):\n",
    "        if prob <= threshold:\n",
    "            cutoff_indices.append(ii)\n",
    "            running_length = tokens_per_sentence_list[ii+1]\n",
    "            \n",
    "        elif running_length + tokens_per_sentence_list[ii+1] > 512:\n",
    "            cutoff_indices.append(ii)\n",
    "            running_length = tokens_per_sentence_list[ii+1]\n",
    "            \n",
    "        else:\n",
    "            running_length += tokens_per_sentence_list[ii+1]\n",
    "        \n",
    "    return cutoff_indices\n",
    "\n",
    "def get_cutoff_indices(text, threshold, nsp_model,tokenizer):\n",
    "    \n",
    "    prob_seq,sentence_list = get_probabilities_on_text_w_NSP(nsp_model, text, tokenizer)\n",
    "    tokens_per_sentence_list = get_tokens_per_sentence_list(tokenizer, sentence_list)\n",
    "    cutoff_indices = apply_threshold(prob_seq, tokens_per_sentence_list, threshold=.5)\n",
    "    \n",
    "    return cutoff_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a05bfa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 8] PrefetchVirtualMemory failed. Detail: [Windows error 8] Not enough memory resources are available to process this command.\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-72db1894b9fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#dataset = load_from_disk('/home/adong/School/NLUProject/data/trivia_qa_rc_tiny')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_from_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\\\wsl$\\Ubuntu-20.04\\home\\jolteon\\NLUProject\\data\\trivia_qa_rc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\datasets-1.2.1-py3.8.egg\\datasets\\load.py\u001b[0m in \u001b[0;36mload_from_disk\u001b[1;34m(dataset_path)\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_from_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dataset_dict.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDatasetDict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_from_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    641\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         raise FileNotFoundError(\n",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\datasets-1.2.1-py3.8.egg\\datasets\\dataset_dict.py\u001b[0m in \u001b[0;36mload_from_disk\u001b[1;34m(dataset_dict_path)\u001b[0m\n\u001b[0;32m    505\u001b[0m             \u001b[1;34m\"splits\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m         ]:\n\u001b[1;32m--> 507\u001b[1;33m             \u001b[0mdataset_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_from_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_dict_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdataset_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\datasets-1.2.1-py3.8.egg\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mload_from_disk\u001b[1;34m(dataset_path)\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdata_file\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_data_files\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_indices_data_files\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m             \u001b[0mdata_file\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"filename\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"filename\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\datasets-1.2.1-py3.8.egg\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m__setstate__\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    413\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace_hist_per_file\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inplace_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m                 \u001b[1;31m# Replay in-place history of transforms (cast_, rename_column_, etc.)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m                 \u001b[0mpa_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m                 \u001b[0msub_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfingerprint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0minplace_transform_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minplace_hist_per_file\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"transforms\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\datasets-1.2.1-py3.8.egg\\datasets\\arrow_reader.py\u001b[0m in \u001b[0;36m_read_files\u001b[1;34m(self, files)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"filename\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mf_dict\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m             \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_dataset_from_filename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[0mpa_tables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mpa_tables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpa_tables\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\datasets-1.2.1-py3.8.egg\\datasets\\arrow_reader.py\u001b[0m in \u001b[0;36m_get_dataset_from_filename\u001b[1;34m(self, filename_skip_take)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[0mmmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mipc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmmap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m         \u001b[0mpa_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m         \u001b[1;31m# here we don't want to slice an empty table, or it may segfault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mskip\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtake\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mskip\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtake\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\pyarrow\\ipc.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.RecordBatchReader.read_all\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\pyarrow\\error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 8] PrefetchVirtualMemory failed. Detail: [Windows error 8] Not enough memory resources are available to process this command.\r\n"
     ]
    }
   ],
   "source": [
    "#dataset = load_from_disk('/home/adong/School/NLUProject/data/trivia_qa_rc_tiny')\n",
    "dataset = load_from_disk(r'\\\\wsl$\\Ubuntu-20.04\\home\\jolteon\\NLUProject\\data\\trivia_qa_rc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc1f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "165bf4ff",
   "metadata": {},
   "source": [
    "# Need to make a dict that is:\n",
    "\n",
    "    entry_id -> [search_context_idx -> splits,  \n",
    "             entity_pages_idx -> splits]  \n",
    "             \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0260223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsp_model = BertForNextSentencePrediction.from_pretrained('bert-base-cased')\n",
    "nsp_model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ef559a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tc_69\n",
      "num entity pages, num search context 0 1\n",
      "tc_261\n",
      "num entity pages, num search context 0 1\n",
      "tc_280\n",
      "num entity pages, num search context 0 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (605 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tc_586\n",
      "num entity pages, num search context 1 0\n",
      "tc_1007\n",
      "num entity pages, num search context 0 1\n",
      "tc_1020\n",
      "num entity pages, num search context 0 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "threshold=.5\n",
    "qid_struct = {}\n",
    "for ii, entry  in enumerate(dataset):\n",
    "    print(entry['question_id'])\n",
    "    print('num entity pages, num search context', len(entry['entity_pages']['wiki_context']),len(entry['search_results']['search_context']))\n",
    "    \n",
    "    wiki_context_splits = []\n",
    "    for context in entry['entity_pages']['wiki_context']:\n",
    "        cutoff_indices = get_cutoff_indices(context, threshold, nsp_model, tokenizer)\n",
    "        wiki_context_splits.append(cutoff_indices)\n",
    "        \n",
    "    search_context_splits = []\n",
    "    for context in entry['search_results']['search_context']:\n",
    "        cutoff_indices = get_cutoff_indices(context, threshold, nsp_model, tokenizer)\n",
    "        search_context_splits.append(cutoff_indices)\n",
    "    \n",
    "    qid_struct[entry['question_id']] = (wiki_context_splits,search_context_splits)\n",
    "    if ii == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33836c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tc_69': ([], [[]]),\n",
       " 'tc_261': ([], [[16]]),\n",
       " 'tc_280': ([],\n",
       "  [[11,\n",
       "    18,\n",
       "    30,\n",
       "    46,\n",
       "    52,\n",
       "    68,\n",
       "    75,\n",
       "    79,\n",
       "    81,\n",
       "    87,\n",
       "    92,\n",
       "    108,\n",
       "    122,\n",
       "    142,\n",
       "    143,\n",
       "    145,\n",
       "    151,\n",
       "    156,\n",
       "    173]]),\n",
       " 'tc_586': ([[13,\n",
       "    15,\n",
       "    27,\n",
       "    33,\n",
       "    49,\n",
       "    65,\n",
       "    76,\n",
       "    89,\n",
       "    105,\n",
       "    120,\n",
       "    135,\n",
       "    153,\n",
       "    166,\n",
       "    173,\n",
       "    189,\n",
       "    208,\n",
       "    226,\n",
       "    240,\n",
       "    254,\n",
       "    257,\n",
       "    259,\n",
       "    269,\n",
       "    284,\n",
       "    301,\n",
       "    320,\n",
       "    322,\n",
       "    323,\n",
       "    338,\n",
       "    355,\n",
       "    374,\n",
       "    391,\n",
       "    406,\n",
       "    418,\n",
       "    432,\n",
       "    444,\n",
       "    462,\n",
       "    474,\n",
       "    485,\n",
       "    501,\n",
       "    517,\n",
       "    535,\n",
       "    554,\n",
       "    563,\n",
       "    575,\n",
       "    591]],\n",
       "  []),\n",
       " 'tc_1007': ([], [[5, 22, 28, 41, 47]]),\n",
       " 'tc_1020': ([], [[10, 15]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qid_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "908405e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': {'aliases': ['My Fair Lady (2010 film)',\n",
       "   'Enry Iggins',\n",
       "   \"Why Can't the English%3F\",\n",
       "   'My Fair Lady',\n",
       "   'My Fair Lady (upcoming film)',\n",
       "   'My Fair Lady (musical)',\n",
       "   'My fair lady',\n",
       "   \"I'm an Ordinary Man\",\n",
       "   'My Fair Lady (2014 film)',\n",
       "   'My Fair Lady (2012 film)',\n",
       "   'My Fair Lady (2015 film)'],\n",
       "  'matched_wiki_entity_name': '',\n",
       "  'normalized_aliases': ['my fair lady musical',\n",
       "   'my fair lady',\n",
       "   'my fair lady 2010 film',\n",
       "   'why can t english 3f',\n",
       "   'my fair lady upcoming film',\n",
       "   'my fair lady 2012 film',\n",
       "   'my fair lady 2014 film',\n",
       "   'my fair lady 2015 film',\n",
       "   'i m ordinary man',\n",
       "   'enry iggins'],\n",
       "  'normalized_matched_wiki_entity_name': '',\n",
       "  'normalized_value': 'my fair lady',\n",
       "  'type': 'WikipediaEntity',\n",
       "  'value': 'My Fair Lady'},\n",
       " 'entity_pages': {'doc_source': [],\n",
       "  'filename': [],\n",
       "  'title': [],\n",
       "  'wiki_context': []},\n",
       " 'question': 'Which musical featured the song The Street Where You Live?',\n",
       " 'question_id': 'tc_261',\n",
       " 'question_source': 'http://www.triviacountry.com/',\n",
       " 'search_results': {'description': ['\"On the Street Where You Live\" is a song with music by Frederick Loewe and lyrics ... \"On the Street Where You Live\" is a song with music by Frederick ...'],\n",
       "  'filename': ['14/14_7295.txt'],\n",
       "  'rank': [5],\n",
       "  'search_context': ['On The Street Where You Live ~ Vic Damone - YouTube\\nOn The Street Where You Live ~ Vic Damone\\nWant to watch this again later?\\nSign in to add this video to a playlist.\\nNeed to report the video?\\nSign in to report inappropriate content.\\nRating is available when the video has been rented.\\nThis feature is not available right now. Please try again later.\\nPublished on Feb 14, 2014\\n\"On the Street Where You Live\" is a song with music by Frederick Loewe and lyrics by Alan Jay Lerner from the 1956 Broadway musical My Fair Lady. It is sung in the musical by the character Freddy Eynsford-Hill, who was portrayed by John Michael King in the original production. In the 1964 film version, it was sung by Bill Shirley, dubbing for actor Jeremy Brett.\\nThe most popular single of the song was recorded by Vic Damone in 1956 for Columbia Records. It reached #4 on the Billboard magazine charts and #6 on Cash Box magazine\\'s chart. It was a #1 hit in the UK in 1958.\\nIn 1955, Damone had only one song on the charts, \"Por Favor,\" which did not make it above #73. However, he did have major roles in two movie musicals, Hit the Deck and Kismet. In early 1956, he moved from Mercury to Columbia Records and had some success on that label with hits like \"On the Street Where You Live\" (from My Fair Lady, his final pop top ten) and \"An Affair to Remember\" (from the movie of the same name). His six original, long-playing albums on Columbia between 1957 and 1961 were That Towering Feeling, Angela Mia, Closer Than a Kiss, This Game of Love, On the Swingin\\' Side and Young and Lively.\\nOh, the towering feeling\\nJust to know somehow you are near\\nI have often walked on this street before\\nBut the pavement always stayed beneath my feet before\\nAll at once am I several stories high\\nKnowing I\\'m on the street where you live\\nAre there lilac trees in the heart of town\\nCan you hear a lark in any other part of town\\nDoes enchantment pour out of every door\\nNo, it\\'s just on the street where you live\\nFor oh, the towering feeling\\nJust to know somehow you are near\\nThe overpowering feeling\\nThat any second you may suddenly appear\\nPeople stop and stare, they don\\'t bother me\\nFor there\\'s nowhere else on earth\\nThat I would rather be\\nLet the time go by, I won\\'t care if I\\nCan be here on the street where you live\\nSound recording administered by SME (thanks for allowing this to remain)\\n\"The AUDIO content does not belong to me. I do not profit from these Videos and/or Slideshows. I do not own copyrights of the images, which are from free websites. This is strictly for Educational use and Commentary purposes.\"\\n\"No copyright is claimed in [the music] and to the extent that material may appear to be infringed, I assert that such alleged infringement is permissible under fair use principles in U.S. copyright laws. If you believe material has been used in an unauthorized manner, please contact the poster.\"\\nCopyright Disclaimer--\"Copyright Disclaimer Under Section 107 of the Copyright Act 1976, allowance is made for \"fair use\" for purposes such as criticism, comment, news reporting, teaching, scholarship, and research. Fair use is a use permitted by copyright statute that might otherwise be infringing. Non-profit, educational or personal use tips the balance in favor of fair use.\"\\nCategory'],\n",
       "  'title': ['On The Street Where You Live ~ Vic Damone - YouTube'],\n",
       "  'url': ['http://www.youtube.com/watch?v=TwhUipIX_oA']}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3163d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Over_BERT(nn.Module):\n",
    "    '''\n",
    "    Input is a entry in trivia_qa dataset\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(LSTM_Over_BERT, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae12ab01",
   "metadata": {},
   "source": [
    "Input is a data entry in trivia_qa dataset.\n",
    "entry contains the question, possible answers, correct answer, and possibly, multiple spans of text?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806372e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e6317c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02886a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04c606b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca77862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
