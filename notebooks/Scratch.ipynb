{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "721c9122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import json\n",
    "import torch\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from torch.nn.functional import softmax\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer,BertForNextSentencePrediction\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5266e3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed182629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fd92754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GeForce RTX 3060 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8024d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = load_from_disk('/home/adong/School/NLUProject/data/trivia_qa_rc_tiny')\n",
    "dataset = load_from_disk(r'\\\\wsl$\\Ubuntu-20.04\\home\\jolteon\\NLUProject\\data\\trivia_qa_rc_tiny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540f5654",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368603df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader = DataLoader(dataset,batch_size=256,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8fcfd2",
   "metadata": {},
   "source": [
    "# Need to make a dict that is:\n",
    "\n",
    "    entry_id -> [search_context_idx -> splits,  \n",
    "             entity_pages_idx -> splits]  \n",
    "             \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539a1db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b428b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "nsp_model = BertForNextSentencePrediction.from_pretrained('bert-base-cased')\n",
    "nsp_model.eval()\n",
    "nsp_model.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7a482bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0cfa84cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextDataset(Dataset):\n",
    "    def __init__(self, sentence_pair_list):\n",
    "        self.sentence_pair_list = sentence_pair_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence_pair_list)\n",
    " \n",
    "    def __getitem__(self,idx):\n",
    "        return(self.sentence_pair_list[idx])\n",
    "\n",
    "def get_probabilities_on_text_w_NSP(nsp_model, text, tokenizer, device):\n",
    "    '''\n",
    "    Returns a sequence of probabilities which represent confidence that the next sentence is part of the same segment\n",
    "    \n",
    "    If text has n sentences, then prob_seq has n-1 probabilities. (If text has 1 sentence, prob_seq is [], the empty list.)\n",
    "    The ii index of prob seq represents the NSP confidence of the ii and ii+1 sentences in text.\n",
    "    Probabilities closer to 1 indicate confidence, Probabilities closer to 0 indicate no confidence.\n",
    "    \n",
    "    '''\n",
    "    sentence_list = sent_tokenize(text)\n",
    "    over_length_indices = []\n",
    "    sentence_pair_list=[]\n",
    "    indices_to_be_processed=[]\n",
    "    #Create Sentence pair list\n",
    "    if len(sentence_list)==1:\n",
    "        return [],sentence_list # Return empty list for probs\n",
    "    \n",
    "    for ii in range(0,len(sentence_list)-1):\n",
    "        sentence_1 = sentence_list[ii]\n",
    "        sentence_2 = sentence_list[ii+1]\n",
    "\n",
    "        #Encode temporarily, just to count\n",
    "        encoded = tokenizer.encode_plus(sentence_1, text_pair=sentence_2, return_tensors='pt')\n",
    "        if encoded['input_ids'].shape[1] > 512: # If two sentences are too long, just split them\n",
    "            over_length_indices.append(ii)\n",
    "        else:# add to list to be processed\n",
    "            indices_to_be_processed.append(ii)\n",
    "            sentence_pair_list.append([sentence_1,sentence_2])\n",
    "    # Now, begin calculating probabilities\n",
    "    with torch.no_grad():\n",
    "        # Load into a dataset and dataloader. We do this for speed\n",
    "        context_dataset = ContextDataset(sentence_pair_list)\n",
    "        context_loader = DataLoader(context_dataset, batch_size=64, shuffle=False, pin_memory=True)\n",
    "        probs_list = [] #Will be list of tensors\n",
    "        for batch_idx,batch in enumerate(context_loader):\n",
    "            if len(batch)==0:\n",
    "                continue\n",
    "            # I swear to god this is a legitimate pytorch bug, but we need to reorganize the batch from the dataloader. Whatever\n",
    "            batch_fixed = [(s1,s2) for s1,s2 in zip(batch[0], batch[1])] \n",
    "            # Batch encode\n",
    "            sentence_pairs = tokenizer.batch_encode_plus(batch_fixed, return_tensors='pt',padding='max_length')\n",
    "            # Run through the model\n",
    "            sentence_pairs.to(device)\n",
    "            logits = nsp_model(**sentence_pairs)[0]\n",
    "            # Get Probability of next sentence.\n",
    "            probs = softmax(logits, dim=1)\n",
    "            probs = probs[:,0]\n",
    "            # Add to list of tensors\n",
    "            probs_list.append(probs)\n",
    "            \n",
    "        #Cat the list of tensors to get a bsize x sequence_length tensors\n",
    "        if len(probs_list)  == 0:\n",
    "            all_probs = []\n",
    "        else:\n",
    "            all_probs = list(torch.cat(probs_list))\n",
    "    # Now, we need to sort the probabilities. Some of probabilities are coming from over_length_indices, some of them are coming from indices_to_be_processed\n",
    "    # We'll zip, then sort, then take the sorted probs.\n",
    "    indices = over_length_indices + indices_to_be_processed\n",
    "    one_probs = [1]*len(over_length_indices)\n",
    "    probs = one_probs + all_probs\n",
    "    probs = [x for _, x in sorted(zip(indices, probs))]\n",
    "    # Return probabilities, and also return sentence list for use later as well\n",
    "    return probs, sentence_list\n",
    "\n",
    "\n",
    "def get_tokens_per_sentence_list(tokenizer,sentence_list):\n",
    "    tokens_per_sentence_list = [len(tokenizer.encode(sentence)) for sentence in sentence_list]\n",
    "    return tokens_per_sentence_list\n",
    "\n",
    "def apply_threshold(prob_seq,tokens_per_sentence_list,threshold):\n",
    "    '''\n",
    "    If prob_seq is empty, we will return and empty list.\n",
    "    '''\n",
    "    # Initialize\n",
    "    cutoff_indices = []\n",
    "    running_length = tokens_per_sentence_list[0]\n",
    "    # \n",
    "    for ii,prob in enumerate(prob_seq):\n",
    "        if prob <= threshold:\n",
    "            cutoff_indices.append(ii)\n",
    "            running_length = tokens_per_sentence_list[ii+1]\n",
    "            \n",
    "        elif running_length + tokens_per_sentence_list[ii+1] > 512:\n",
    "            cutoff_indices.append(ii)\n",
    "            running_length = tokens_per_sentence_list[ii+1]\n",
    "            \n",
    "        else:\n",
    "            running_length += tokens_per_sentence_list[ii+1]\n",
    "        \n",
    "    return cutoff_indices\n",
    "\n",
    "def get_cutoff_indices(text, threshold, nsp_model,tokenizer, device):\n",
    "    \n",
    "    prob_seq, sentence_list = get_probabilities_on_text_w_NSP(nsp_model, text, tokenizer,device)\n",
    "    tokens_per_sentence_list = get_tokens_per_sentence_list(tokenizer, sentence_list)\n",
    "    cutoff_indices = apply_threshold(prob_seq, tokens_per_sentence_list, threshold=.5)\n",
    "    \n",
    "    return cutoff_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9a805efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6ecbede8d94fc6843ecf001dff5bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started:  0\n",
      "started:  1\n",
      "started:  2\n",
      "started:  3\n",
      "started:  4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "threshold=.5\n",
    "qid_struct = {}\n",
    "key='test'\n",
    "for ii, entry  in tqdm(enumerate(dataset)):\n",
    "    if ii ==5:\n",
    "        break\n",
    "    print('started: ',str(ii))\n",
    "\n",
    "    if len(entry['entity_pages']['wiki_context'])==0:\n",
    "        wiki_context_probs = None\n",
    "    else:\n",
    "        wiki_context_probs = []\n",
    "        for context in entry['entity_pages']['wiki_context']:\n",
    "            prob_seq , _ = get_probabilities_on_text_w_NSP(nsp_model, context, tokenizer, device)\n",
    "            wiki_context_probs.append(prob_seq)\n",
    "            \n",
    "    if len(entry['search_results']['search_context']) == 0:\n",
    "         search_context_probs = None\n",
    "    else:\n",
    "        search_context_probs = []\n",
    "        for context in entry['search_results']['search_context']:\n",
    "\n",
    "            prob_seq , _ = get_probabilities_on_text_w_NSP(nsp_model, context, tokenizer, device)\n",
    "            search_context_probs.append(prob_seq)\n",
    "    \n",
    "    qid_struct[entry['question_id']] = (wiki_context_probs,search_context_probs)\n",
    "file_name = key + '_qid_struct.pkl'\n",
    "\n",
    "with open(r\"\\\\wsl$\\Ubuntu-20.04\\home\\jolteon\\NLUProject\\\\\" + file_name, 'wb') as handle:\n",
    "    pickle.dump(qid_struct, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f4ce5",
   "metadata": {},
   "source": [
    "# Cutoff index Sequence version of for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3f6f3e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6d5b01f1a94055b4b63536cbda9a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started:  0\n",
      "started:  1\n",
      "started:  2\n",
      "started:  3\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-7b8ca172c002>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mwiki_context_splits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcontext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entity_pages'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'wiki_context'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mcutoff_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_cutoff_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnsp_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0mwiki_context_splits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcutoff_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-de84de941a39>\u001b[0m in \u001b[0;36mget_cutoff_indices\u001b[1;34m(text, threshold, nsp_model, tokenizer, device)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_cutoff_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnsp_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m     \u001b[0mprob_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_probabilities_on_text_w_NSP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnsp_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m     \u001b[0mtokens_per_sentence_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tokens_per_sentence_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[0mcutoff_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_threshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens_per_sentence_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-de84de941a39>\u001b[0m in \u001b[0;36mget_probabilities_on_text_w_NSP\u001b[1;34m(nsp_model, text, tokenizer, device)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0msentence_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_encode_plus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_fixed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max_length'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;31m# Run through the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0msentence_pairs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnsp_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0msentence_pairs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;31m# Get Probability of next sentence.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\transformers-4.4.2-py3.8.egg\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1430\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1431\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Method `{func.__name__}` requires PyTorch.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\transformers-4.4.2-py3.8.egg\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    735\u001b[0m         \u001b[1;31m# into a HalfTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_is_torch_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    738\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Attempting to cast a BatchEncoding to type {str(device)}. This is not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\transformers-4.4.2-py3.8.egg\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    735\u001b[0m         \u001b[1;31m# into a HalfTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_is_torch_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    738\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Attempting to cast a BatchEncoding to type {str(device)}. This is not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "threshold=.5\n",
    "qid_struct = {}\n",
    "key='test'\n",
    "for ii, entry  in tqdm(enumerate(dataset)):\n",
    "    print('started: ',str(ii))\n",
    "\n",
    "    if len(entry['entity_pages']['wiki_context'])==0:\n",
    "        wiki_context_splits = None\n",
    "    else:\n",
    "        wiki_context_splits = []\n",
    "        for context in entry['entity_pages']['wiki_context']:\n",
    "            cutoff_indices = get_cutoff_indices(context, threshold, nsp_model, tokenizer,device)\n",
    "            wiki_context_splits.append(cutoff_indices)\n",
    "            \n",
    "    if len(entry['search_results']['search_context']) == 0:\n",
    "         search_context_splits = None\n",
    "    else:\n",
    "        search_context_splits = []\n",
    "        for context in entry['search_results']['search_context']:\n",
    "\n",
    "            cutoff_indices = get_cutoff_indices(context, threshold, nsp_model, tokenizer,device)\n",
    "            search_context_splits.append(cutoff_indices)\n",
    "    \n",
    "    qid_struct[entry['question_id']] = (wiki_context_splits,search_context_splits)\n",
    "file_name = key + '_qid_struct.pkl'\n",
    "\n",
    "with open(r\"\\\\wsl$\\Ubuntu-20.04\\home\\jolteon\\NLUProject\\\\\" + file_name, 'wb') as handle:\n",
    "    pickle.dump(qid_struct, handle, protocol=pickle.HIGHEST_PROTOCOL)   \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0ef6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "   with open(r\"\\\\wsl$\\Ubuntu-20.04\\home\\jolteon\\NLUProject\\\\\" + file_name, 'wb') as handle:\n",
    "            pickle.dump(qid_struct, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cdd2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794f9010",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Over_BERT(nn.Module):\n",
    "    '''\n",
    "    Input is a entry in trivia_qa dataset\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(LSTM_Over_BERT, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb45469e",
   "metadata": {},
   "source": [
    "Input is a data entry in trivia_qa dataset.\n",
    "entry contains the question, possible answers, correct answer, and possibly, multiple spans of text?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ffc9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c3fc01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f74875d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaa343a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f15dae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
