{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecc2b4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import json\n",
    "import torch\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from torch.nn.functional import softmax\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer,BertForNextSentencePrediction\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bd3961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3183aafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d2cc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GeForce RTX 3060 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05ce24ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = load_from_disk('/home/adong/School/NLUProject/data/trivia_qa_rc_tiny')\n",
    "dataset = load_from_disk(r'\\\\wsl$\\Ubuntu-20.04\\home\\jolteon\\NLUProject\\data\\trivia_qa_rc_tiny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e696f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b3a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader = DataLoader(dataset,batch_size=256,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aca356",
   "metadata": {},
   "source": [
    "# Need to make a dict that is:\n",
    "\n",
    "    entry_id -> [search_context_idx -> splits,  \n",
    "             entity_pages_idx -> splits]  \n",
    "             \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2827e595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a11a7670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56eabd6941946cfab612484231afab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nsp_model = BertForNextSentencePrediction.from_pretrained('prajjwal1/bert-small')\n",
    "nsp_model.eval()\n",
    "nsp_model.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-small')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e2e2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3504888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextDataset(Dataset):\n",
    "    def __init__(self, sentence_pair_list):\n",
    "        self.sentence_pair_list = sentence_pair_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence_pair_list)\n",
    " \n",
    "    def __getitem__(self,idx):\n",
    "        return(self.sentence_pair_list[idx])\n",
    "\n",
    "def get_probabilities_on_text_w_NSP(nsp_model, text, tokenizer, device):\n",
    "    '''\n",
    "    Returns a sequence of probabilities which represent confidence that the next sentence is part of the same segment\n",
    "    \n",
    "    If text has n sentences, then prob_seq has n-1 probabilities. (If text has 1 sentence, prob_seq is [], the empty list.)\n",
    "    The ii index of prob seq represents the NSP confidence of the ii and ii+1 sentences in text.\n",
    "    Probabilities closer to 1 indicate confidence, Probabilities closer to 0 indicate no confidence.\n",
    "    \n",
    "    '''\n",
    "    #print(text)\n",
    "    text = text.lower()\n",
    "    #print(text)\n",
    "    sentence_list = sent_tokenize(text)\n",
    "    over_length_indices = []\n",
    "    sentence_pair_list=[]\n",
    "    indices_to_be_processed=[]\n",
    "    #Create Sentence pair list\n",
    "    if len(sentence_list)==1:\n",
    "        return [],sentence_list # Return empty list for probs\n",
    "    \n",
    "    for ii in range(0,len(sentence_list)-1):\n",
    "        sentence_1 = sentence_list[ii]\n",
    "        sentence_2 = sentence_list[ii+1]\n",
    "\n",
    "        #Encode temporarily, just to count\n",
    "        encoded = tokenizer.encode_plus(sentence_1, text_pair=sentence_2, return_tensors='pt')\n",
    "        if encoded['input_ids'].shape[1] > 512: # If two sentences are too long, just split them\n",
    "            over_length_indices.append(ii)\n",
    "        else:# add to list to be processed\n",
    "            indices_to_be_processed.append(ii)\n",
    "            sentence_pair_list.append([sentence_1,sentence_2])\n",
    "    # Now, begin calculating probabilities\n",
    "    with torch.no_grad():\n",
    "        # Load into a dataset and dataloader. We do this for speed\n",
    "        context_dataset = ContextDataset(sentence_pair_list)\n",
    "        context_loader = DataLoader(context_dataset, batch_size=64, shuffle=False, pin_memory=True)\n",
    "        probs_list = [] #Will be list of tensors\n",
    "        for batch_idx,batch in enumerate(context_loader):\n",
    "            if len(batch)==0:\n",
    "                continue\n",
    "            # I swear to god this is a legitimate pytorch bug, but we need to reorganize the batch from the dataloader. Whatever\n",
    "            batch_fixed = [(s1,s2) for s1,s2 in zip(batch[0], batch[1])] \n",
    "            # Batch encode\n",
    "            sentence_pairs = tokenizer.batch_encode_plus(batch_fixed, return_tensors='pt',padding=True)\n",
    "            # Run through the model\n",
    "            sentence_pairs.to(device)\n",
    "            logits = nsp_model(**sentence_pairs)[0]\n",
    "            # Get Probability of next sentence.\n",
    "            probs = softmax(logits, dim=1)\n",
    "            probs = probs[:,0]\n",
    "            # Add to list of tensors\n",
    "            probs_list.append(probs)\n",
    "            \n",
    "        #Cat the list of tensors to get a bsize x sequence_length tensors\n",
    "        if len(probs_list)  == 0:\n",
    "            all_probs = []\n",
    "        else:\n",
    "            all_probs = list(torch.cat(probs_list))\n",
    "    # Now, we need to sort the probabilities. Some of probabilities are coming from over_length_indices, some of them are coming from indices_to_be_processed\n",
    "    # We'll zip, then sort, then take the sorted probs.\n",
    "    indices = over_length_indices + indices_to_be_processed\n",
    "    one_probs = [1]*len(over_length_indices)\n",
    "    probs = one_probs + all_probs\n",
    "    probs = [x for _, x in sorted(zip(indices, probs))]\n",
    "    # Return probabilities, and also return sentence list for use later as well\n",
    "    return probs, sentence_list\n",
    "\n",
    "\n",
    "def get_tokens_per_sentence_list(tokenizer,sentence_list):\n",
    "    tokens_per_sentence_list = [len(tokenizer.encode(sentence)) for sentence in sentence_list]\n",
    "    return tokens_per_sentence_list\n",
    "\n",
    "def apply_threshold(prob_seq,tokens_per_sentence_list,threshold):\n",
    "    '''\n",
    "    If prob_seq is empty, we will return and empty list.\n",
    "    '''\n",
    "    # Initialize\n",
    "    cutoff_indices = []\n",
    "    running_length = tokens_per_sentence_list[0]\n",
    "    # \n",
    "    for ii,prob in enumerate(prob_seq):\n",
    "        if prob <= threshold:\n",
    "            cutoff_indices.append(ii)\n",
    "            running_length = tokens_per_sentence_list[ii+1]\n",
    "            \n",
    "        elif running_length + tokens_per_sentence_list[ii+1] > 512:\n",
    "            cutoff_indices.append(ii)\n",
    "            running_length = tokens_per_sentence_list[ii+1]\n",
    "            \n",
    "        else:\n",
    "            running_length += tokens_per_sentence_list[ii+1]\n",
    "        \n",
    "    return cutoff_indices\n",
    "\n",
    "def get_cutoff_indices(text, threshold, nsp_model,tokenizer, device):\n",
    "    \n",
    "    prob_seq, sentence_list = get_probabilities_on_text_w_NSP(nsp_model, text, tokenizer,device)\n",
    "    tokens_per_sentence_list = get_tokens_per_sentence_list(tokenizer, sentence_list)\n",
    "    cutoff_indices = apply_threshold(prob_seq, tokens_per_sentence_list, threshold=.5)\n",
    "    \n",
    "    return cutoff_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8241011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5bacb76f6547478cd03fc61dccb2e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started:  0\n",
      "started:  1\n",
      "started:  2\n",
      "started:  3\n",
      "started:  4\n",
      "started:  5\n",
      "started:  6\n",
      "started:  7\n",
      "started:  8\n",
      "started:  9\n",
      "started:  10\n",
      "started:  11\n",
      "started:  12\n",
      "started:  13\n",
      "started:  14\n",
      "started:  15\n",
      "started:  16\n",
      "started:  17\n",
      "started:  18\n",
      "started:  19\n",
      "started:  20\n",
      "started:  21\n",
      "started:  22\n",
      "started:  23\n",
      "started:  24\n",
      "started:  25\n",
      "started:  26\n",
      "started:  27\n",
      "started:  28\n",
      "started:  29\n",
      "started:  30\n",
      "started:  31\n",
      "started:  32\n",
      "started:  33\n",
      "started:  34\n",
      "started:  35\n",
      "started:  36\n",
      "started:  37\n",
      "started:  38\n",
      "started:  39\n",
      "started:  40\n",
      "started:  41\n",
      "started:  42\n",
      "started:  43\n",
      "started:  44\n",
      "started:  45\n",
      "started:  46\n",
      "started:  47\n",
      "started:  48\n",
      "started:  49\n",
      "started:  50\n",
      "started:  51\n",
      "started:  52\n",
      "started:  53\n",
      "started:  54\n",
      "started:  55\n",
      "started:  56\n",
      "started:  57\n",
      "started:  58\n",
      "started:  59\n",
      "started:  60\n",
      "started:  61\n",
      "started:  62\n",
      "started:  63\n",
      "started:  64\n",
      "started:  65\n",
      "started:  66\n",
      "started:  67\n",
      "started:  68\n",
      "started:  69\n",
      "started:  70\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-2e122bb799a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcontext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'search_results'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'search_context'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mprob_seq\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_probabilities_on_text_w_NSP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnsp_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0msearch_context_probs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-acb961c370f2>\u001b[0m in \u001b[0;36mget_probabilities_on_text_w_NSP\u001b[1;34m(nsp_model, text, tokenizer, device)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mbatch_fixed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;31m# Batch encode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0msentence_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_encode_plus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_fixed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m             \u001b[1;31m# Run through the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0msentence_pairs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\transformers-4.4.2-py3.8.egg\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2432\u001b[0m         )\n\u001b[0;32m   2433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2434\u001b[1;33m         return self._batch_encode_plus(\n\u001b[0m\u001b[0;32m   2435\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2436\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\transformers-4.4.2-py3.8.egg\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[0mfirst_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[0msecond_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpair_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecond_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\transformers-4.4.2-py3.8.egg\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m                 \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\transformers-4.4.2-py3.8.egg\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[0mno_split_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[0mtokenized_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_on_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_split_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\transformers-4.4.2-py3.8.egg\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36msplit_on_tokens\u001b[1;34m(tok_list, text)\u001b[0m\n\u001b[0;32m    330\u001b[0m                 \u001b[0mtext_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m             return list(\n\u001b[0m\u001b[0;32m    333\u001b[0m                 itertools.chain.from_iterable(\n\u001b[0;32m    334\u001b[0m                     (\n",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\transformers-4.4.2-py3.8.egg\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    333\u001b[0m                 itertools.chain.from_iterable(\n\u001b[0;32m    334\u001b[0m                     (\n\u001b[1;32m--> 335\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m                     )\n",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\transformers-4.4.2-py3.8.egg\\transformers\\models\\bert\\tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m                 \u001b[1;31m# If the token is part of the never_split set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\transformers-4.4.2-py3.8.egg\\transformers\\models\\bert\\tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, never_split)\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;31m# union() returns a new set by concatenating the two sets.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[0mnever_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnever_split\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m         \u001b[1;31m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\transformers-4.4.2-py3.8.egg\\transformers\\models\\bert\\tokenization_bert.py\u001b[0m in \u001b[0;36m_clean_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    493\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m             \u001b[0mcp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 495\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mcp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0xFFFD\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_is_control\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    496\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0m_is_whitespace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "threshold=.5\n",
    "qid_struct = {}\n",
    "key='test'\n",
    "for ii, entry  in tqdm(enumerate(dataset)):\n",
    "    if ii ==100:\n",
    "        break\n",
    "    print('started: ',str(ii))\n",
    "\n",
    "    if len(entry['entity_pages']['wiki_context'])==0:\n",
    "        wiki_context_probs = None\n",
    "    else:\n",
    "        wiki_context_probs = []\n",
    "        for context in entry['entity_pages']['wiki_context']:\n",
    "            prob_seq , _ = get_probabilities_on_text_w_NSP(nsp_model, context, tokenizer, device)\n",
    "            wiki_context_probs.append(prob_seq)\n",
    "            \n",
    "    if len(entry['search_results']['search_context']) == 0:\n",
    "         search_context_probs = None\n",
    "    else:\n",
    "        search_context_probs = []\n",
    "        for context in entry['search_results']['search_context']:\n",
    "\n",
    "            prob_seq , _ = get_probabilities_on_text_w_NSP(nsp_model, context, tokenizer, device)\n",
    "            search_context_probs.append(prob_seq)\n",
    "    \n",
    "    qid_struct[entry['question_id']] = (wiki_context_probs,search_context_probs)\n",
    "file_name = key + '_qid_struct.pkl'\n",
    "\n",
    "with open(r\"\\\\wsl$\\Ubuntu-20.04\\home\\jolteon\\NLUProject\\\\\" + file_name, 'wb') as handle:\n",
    "    pickle.dump(qid_struct, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432d6f26",
   "metadata": {},
   "source": [
    "# Cutoff index Sequence version of for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "081fc979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6d5b01f1a94055b4b63536cbda9a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started:  0\n",
      "started:  1\n",
      "started:  2\n",
      "started:  3\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-7b8ca172c002>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mwiki_context_splits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcontext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entity_pages'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'wiki_context'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mcutoff_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_cutoff_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnsp_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0mwiki_context_splits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcutoff_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-de84de941a39>\u001b[0m in \u001b[0;36mget_cutoff_indices\u001b[1;34m(text, threshold, nsp_model, tokenizer, device)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_cutoff_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnsp_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m     \u001b[0mprob_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_probabilities_on_text_w_NSP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnsp_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m     \u001b[0mtokens_per_sentence_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tokens_per_sentence_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[0mcutoff_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_threshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens_per_sentence_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-de84de941a39>\u001b[0m in \u001b[0;36mget_probabilities_on_text_w_NSP\u001b[1;34m(nsp_model, text, tokenizer, device)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0msentence_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_encode_plus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_fixed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max_length'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;31m# Run through the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0msentence_pairs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnsp_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0msentence_pairs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;31m# Get Probability of next sentence.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\transformers-4.4.2-py3.8.egg\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1430\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1431\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Method `{func.__name__}` requires PyTorch.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\transformers-4.4.2-py3.8.egg\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    735\u001b[0m         \u001b[1;31m# into a HalfTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_is_torch_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    738\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Attempting to cast a BatchEncoding to type {str(device)}. This is not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\miniconda3\\envs\\NLUProject\\lib\\site-packages\\transformers-4.4.2-py3.8.egg\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    735\u001b[0m         \u001b[1;31m# into a HalfTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_is_torch_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    738\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Attempting to cast a BatchEncoding to type {str(device)}. This is not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "threshold=.5\n",
    "qid_struct = {}\n",
    "key='test'\n",
    "for ii, entry  in tqdm(enumerate(dataset)):\n",
    "    print('started: ',str(ii))\n",
    "\n",
    "    if len(entry['entity_pages']['wiki_context'])==0:\n",
    "        wiki_context_splits = None\n",
    "    else:\n",
    "        wiki_context_splits = []\n",
    "        for context in entry['entity_pages']['wiki_context']:\n",
    "            cutoff_indices = get_cutoff_indices(context, threshold, nsp_model, tokenizer,device)\n",
    "            wiki_context_splits.append(cutoff_indices)\n",
    "            \n",
    "    if len(entry['search_results']['search_context']) == 0:\n",
    "         search_context_splits = None\n",
    "    else:\n",
    "        search_context_splits = []\n",
    "        for context in entry['search_results']['search_context']:\n",
    "\n",
    "            cutoff_indices = get_cutoff_indices(context, threshold, nsp_model, tokenizer,device)\n",
    "            search_context_splits.append(cutoff_indices)\n",
    "    \n",
    "    qid_struct[entry['question_id']] = (wiki_context_splits,search_context_splits)\n",
    "file_name = key + '_qid_struct.pkl'\n",
    "\n",
    "with open(r\"\\\\wsl$\\Ubuntu-20.04\\home\\jolteon\\NLUProject\\\\\" + file_name, 'wb') as handle:\n",
    "    pickle.dump(qid_struct, handle, protocol=pickle.HIGHEST_PROTOCOL)   \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b6617",
   "metadata": {},
   "outputs": [],
   "source": [
    "   with open(r\"\\\\wsl$\\Ubuntu-20.04\\home\\jolteon\\NLUProject\\\\\" + file_name, 'wb') as handle:\n",
    "            pickle.dump(qid_struct, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9edb91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811d8f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Over_BERT(nn.Module):\n",
    "    '''\n",
    "    Input is a entry in trivia_qa dataset\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(LSTM_Over_BERT, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65887e5d",
   "metadata": {},
   "source": [
    "Input is a data entry in trivia_qa dataset.\n",
    "entry contains the question, possible answers, correct answer, and possibly, multiple spans of text?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d589f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e838b92f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca96a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8bb4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c67ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
